{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 참조 깃헙\n",
    "    * https://github.com/GunhoChoi/PyTorch-FastCampus/blob/master/07_Transfer_Learning/0_Pretrained_Basic/Transfer_Learning.ipynb\n",
    "    * 모델 하나로 만들자.\n",
    "    * 01_pretrainedVGG_media_v4\n",
    "* MediaClassifier_a_VGG\n",
    "    * 다른 네트워크의 base 가 되도록 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1. Settings\n",
    "### 1) Important required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torch.utils.data as data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size= 16 #64  #1\n",
    "learning_rate = 0.0001\n",
    "epoch = 50\n",
    "\n",
    "n_node = 1024  # customized last layer 의 노드 수. 64, 128, 256, 512, 1024\n",
    "dropratio = 0.5   # 얼마나 드랍시킬지 inverse keepratio \n",
    "\n",
    "imgsize = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loader\n",
    "### 트레이닝 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/r320ws/anaconda3/envs/joono/lib/python3.8/site-packages/torchvision/transforms/transforms.py:285: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../../images/painting_dataset/real_artwork_divided_shffl_4K/Train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-44a68b11834a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimg_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../../../images/painting_dataset/real_artwork_divided_shffl_4K/Train\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m train_data = dset.ImageFolder(img_dir, transforms.Compose([      \n\u001b[0m\u001b[1;32m      3\u001b[0m             \u001b[0;31m# ①(512)③②RCrop  <-- Best !!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCenterCrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgsize\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m     \u001b[0;31m# ① CenterCrop(512)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomCrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m       \u001b[0;31m# ③ RandomCrop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/r320ws/anaconda3/envs/joono/lib/python3.8/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0mis_valid_file\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     ):\n\u001b[0;32m--> 253\u001b[0;31m         super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS if is_valid_file is None else None,\n\u001b[0m\u001b[1;32m    254\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m                                           \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/r320ws/anaconda3/envs/joono/lib/python3.8/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    124\u001b[0m         super(DatasetFolder, self).__init__(root, transform=transform,\n\u001b[1;32m    125\u001b[0m                                             target_transform=target_transform)\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_find_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/r320ws/anaconda3/envs/joono/lib/python3.8/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m_find_classes\u001b[0;34m(self, dir)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mNo\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0msubdirectory\u001b[0m \u001b[0mof\u001b[0m \u001b[0manother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \"\"\"\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mcls_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../../images/painting_dataset/real_artwork_divided_shffl_4K/Train'"
     ]
    }
   ],
   "source": [
    "img_dir = \"../../../images/painting_dataset/real_artwork_divided_shffl_4K/Train\"\n",
    "train_data = dset.ImageFolder(img_dir, transforms.Compose([      \n",
    "            # ①(512)③②RCrop  <-- Best !!\n",
    "            transforms.CenterCrop(imgsize*2),     # ① CenterCrop(512)\n",
    "            transforms.RandomCrop(imgsize),       # ③ RandomCrop\n",
    "            transforms.RandomHorizontalFlip(),    # ② RandomHorizontalFlip\n",
    "    \n",
    "            transforms.Scale(imgsize),\n",
    "            transforms.ToTensor()\n",
    "            ]))\n",
    "print(train_data.__len__())\n",
    "\n",
    "train_batch = data.DataLoader(train_data, batch_size=batch_size,\n",
    "                            shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 고정된 데이터 셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Dev data\n",
    "img_dir = \"../../../images/painting_dataset/real_artwork_divided_shffl_4K/Valid\"\n",
    "dev_data = dset.ImageFolder(img_dir, transforms.Compose([\n",
    "            #transforms.Scale(256),\n",
    "            #transforms.RandomSizedCrop(224),\n",
    "    \n",
    "            transforms.CenterCrop(size=imgsize),\n",
    "            transforms.Scale(imgsize),\n",
    "            transforms.ToTensor()\n",
    "            ]))\n",
    "dev_batch = data.DataLoader(dev_data, batch_size=batch_size,\n",
    "                            shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "# 3. Test data\n",
    "img_dir = \"../../../images/painting_dataset/real_artwork_divided_shffl_4K/Test\"\n",
    "test_data = dset.ImageFolder(img_dir, transforms.Compose([\n",
    "            #transforms.Scale(256),\n",
    "            #transforms.RandomSizedCrop(224),\n",
    "    \n",
    "            transforms.CenterCrop(size=imgsize),\n",
    "            transforms.Scale(imgsize),\n",
    "            transforms.ToTensor()\n",
    "            ]))\n",
    "test_batch = data.DataLoader(test_data, batch_size=batch_size,\n",
    "                            shuffle=True, num_workers=2)\n",
    "        \n",
    "nclass = len(train_data.classes)\n",
    "print(\"# of classes: %d\" %nclass)\n",
    "print(train_data.classes)\n",
    "print(train_data.class_to_idx)\n",
    "print(train_data.__len__())\n",
    "\n",
    "print(\"Training: %d, Dev: %d, Test: %d,\" %(train_data.__len__(), dev_data.__len__(), test_data.__len__())),\n",
    "\n",
    "# for imgs, labels in train_batch:\n",
    "#     for j in range(len(imgs)):\n",
    "#         img = transforms.ToPILImage()(imgs[j])\n",
    "#         plt.title(\"label: %d\" % labels[j])\n",
    "#         plt.imshow(img)\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model\n",
    "### 1) Pretrained VGG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vgg = models.vgg19(pretrained=True)\n",
    "\n",
    "for name,module in vgg.named_children():\n",
    "    print(name)\n",
    "\n",
    "print(list(vgg.children())[0])\n",
    "print(list(vgg.children())[-1])\n",
    "\n",
    "# cnt = 0\n",
    "# for i in model.children():\n",
    "#     print(\"yhk[%d]\" %cnt),\n",
    "#     print(i)\n",
    "#     cnt = cnt+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Customized Fully Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dim = 64\n",
    "fsize = imgsize/32\n",
    "\n",
    "class MyVGG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyVGG, self).__init__()\n",
    "        self.layer0 = nn.Sequential(*list(vgg.children())[0])  # [0]: features(conv), [1]: classifier(fc)\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(8*base_dim * fsize * fsize, n_node),\n",
    "            nn.BatchNorm1d(n_node),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(dropratio),  # 0.3 만큼 drop 하자.\n",
    "            \n",
    "            nn.Linear(n_node, n_node),\n",
    "            nn.BatchNorm1d(n_node),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(dropratio),\n",
    "            \n",
    "            nn.Linear(n_node, n_node),\n",
    "            nn.BatchNorm1d(n_node),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(dropratio),\n",
    "            \n",
    "            nn.Linear(n_node, nclass),\n",
    "        )\n",
    "        # weight initialization\n",
    "        for m in self.layer1.modules():\n",
    "            #print(m)\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal(m.weight.data)  # REUL 일 때\n",
    "                m.bias.data.fill_(0)\n",
    "            if isinstance(m, nn.Linear):\n",
    "                init.kaiming_normal(m.weight.data)\n",
    "                m.bias.data.fill_(0)\n",
    "    def forward(self, x):\n",
    "        #print(x.size())  # layer0의 사이즈를 무식하게 프린트 하여 알아낼 수 있음(batchsize, x,x,x)\n",
    "        out = self.layer0(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.layer1(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Model on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyVGG().cuda()\n",
    "\n",
    "for params in model.layer0.parameters():\n",
    "    params.required_grad = False\n",
    "    \n",
    "for params in model.layer1.parameters():\n",
    "    params.required_grad = True\n",
    "    \n",
    "\n",
    "# fc_layer = MyFullyConn().cuda()\n",
    "# cnt = 0\n",
    "# for param in fc_layer.parameters():\n",
    "#     print(\"[%d]\" %cnt),\n",
    "#     print(param.requires_grad)\n",
    "#     cnt = cnt+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for name in model.children():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optimizer & Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.layer1.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import utils\n",
    "\n",
    "# total_time = 0\n",
    "# disp_step = 10\n",
    "\n",
    "# # 재시작하지 않는 다면\n",
    "# for i in range(2):\n",
    "#     print(\"%d..\" %i),\n",
    "#     for img,label in train_batch:\n",
    "#         img = Variable(img).cuda()\n",
    "#         label = Variable(label).cuda()\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         output = model(img)\n",
    "#         print \"output size:\", output.size()\n",
    "#         print \"label size:\", label.size()\n",
    "#         loss = loss_func(output,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "total_time = 0\n",
    "disp_step = 10\n",
    "\n",
    "to_train = True\n",
    "if (to_train==False):\n",
    "    #netname = './nets/media_vgg19_fixed.pkl'\n",
    "    #netname = './nets/media_vgg19_RCrop_fixed.pkl'\n",
    "    netname = './nets/media_vgg19_50.pkl'\n",
    "    model = torch.load(netname)\n",
    "else:   \n",
    "    print(\"3 layer, n_node: %d, dropratio: %.2f\" %(n_node, dropratio))\n",
    "    model.eval()  # evaluation(test) mode 로 바꾸기 -> dropout, batch normalization 에 영향을 줌.\n",
    "    train_corr = utils.ComputeCorr(train_batch, model)\n",
    "    dev_corr = utils.ComputeCorr(dev_batch, model)\n",
    "    test_corr = utils.ComputeCorr(test_batch, model)\n",
    "    print(\"Correct of train: %.2f, dev: %.2f, test: %.2f\" \n",
    "          %(train_corr, dev_corr, test_corr))\n",
    "    model.train()\n",
    "    \n",
    "    netname = './nets/media_vgg19'\n",
    "\n",
    "    # graph 그리기\n",
    "    x_epoch = []\n",
    "    y_train_err = []\n",
    "    y_dev_err = []\n",
    "    y_test_err = []\n",
    "    \n",
    "    x_epoch.append(0)\n",
    "    y_train_err.append(100.0-train_corr)\n",
    "    y_dev_err.append(100.0-dev_corr)\n",
    "    y_test_err.append(100.0-test_corr)\n",
    "    \n",
    "#     # 학습을 재시작한다면\n",
    "#     netname = '../nets/media_pre_vgg19.pkl'\n",
    "#     model = torch.load(netname)\n",
    "#     # 파라미터 학습 여부 결정\n",
    "#     for params in model.layer0.parameters():\n",
    "#         params.required_grad = False    \n",
    "#     for params in model.layer1.parameters():\n",
    "#         params.required_grad = True\n",
    "#     for i in range(34, epoch):\n",
    "    \n",
    "    # 재시작하지 않는 다면\n",
    "    for i in range(epoch):\n",
    "        start_time = time.time()\n",
    "        print(\"%d..\" %i),\n",
    "        for img,label in train_batch:\n",
    "            img = Variable(img).cuda()\n",
    "            label = Variable(label).cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(img)\n",
    "            loss = loss_func(output,label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        end_time = time.time()\n",
    "        duration = end_time - start_time\n",
    "        total_time += duration\n",
    "        if (i % disp_step == 0) or (i==epoch-1):\n",
    "            torch.save(model, netname+'_%d.pkl'%i, )\n",
    "            print(\"\\n[%d/%d] loss: %.3f, \" %(i, epoch, (loss.cpu()).data.numpy())),\n",
    "            \n",
    "            # train, dev, train accr\n",
    "            model.eval()  # evaluation(test) mode 로 바꾸기 -> dropout, batch normalization 에 영향을 줌.\n",
    "            train_corr = utils.ComputeCorr(train_batch, model)\n",
    "            dev_corr = utils.ComputeCorr(dev_batch, model)\n",
    "            test_corr = utils.ComputeCorr(test_batch, model)\n",
    "            print(\"Correct of train: %.2f, dev: %.2f, test: %.2f, \" \n",
    "                  %(train_corr, dev_corr, test_corr)),\n",
    "            model.train()            \n",
    "            print(\"time: %.2f sec..\" %(total_time))\n",
    "            \n",
    "            # graph 그리기\n",
    "            x_epoch.append(i+1)\n",
    "            y_train_err.append(100.0-train_corr)\n",
    "            y_dev_err.append(100.0-dev_corr)\n",
    "            y_test_err.append(100.0-test_corr)\n",
    "    print(\"Total time: %.2f sec\" %total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch-err curve \n",
    "if (to_train):\n",
    "    plt.plot(x_epoch, y_train_err, color='black', label='train err', linestyle='--')\n",
    "    plt.plot(x_epoch, y_dev_err, color='red', label='dev err')\n",
    "    plt.plot(x_epoch, y_test_err, color='blue', label='test err')\n",
    "    \n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('err')\n",
    "    plt.title('epoch & err graph')\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation for dev & test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.eval()   # evaluation(test) mode 로 바꾸기 -> dropout, batch normalization 에 영향을 줌.\n",
    "utils.EvaluateClassifier(dev_batch, model, dev_data.classes, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "_, _,_ = utils.EvaluateClassifier(test_batch, model, test_data.classes, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.VisTFPred(test_batch, model, test_data.classes, batch_size, i_n=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "joono_py38",
   "language": "python",
   "name": "joono_py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
